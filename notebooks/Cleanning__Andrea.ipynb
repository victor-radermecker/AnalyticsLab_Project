{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\## NOTE\n",
    "this file takes as input the file made in data_cleaning, and creates a new file further cleaned\n",
    "# \\###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "`Author: Andrea Zanon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# identify digits from text\n",
    "from string import digits\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# nltk, useful to clean text (may be needed to donwload files if using for first time)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# detect language from text\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victor/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/victor/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/azanon/ML_NOAC_NOVA_Extraction.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename LastIncomingEmail__c in LastIncomingEmail\n",
    "data = data.rename(columns={\"LastIncomingEmail__c\": \"LastIncomingEmail\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on copy on data\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaseNumber                      0\n",
       "Type                            0\n",
       "Topics                       1078\n",
       "TeamName                        0\n",
       "RequesterEmail               5076\n",
       "EmailObject                  1417\n",
       "LastEmailCCAddress          90924\n",
       "AttributesURL                   0\n",
       "ContactAttributesURL        24360\n",
       "ContactEmail                  344\n",
       "LastIncomingEmailContent       17\n",
       "LastEmailCCAddressCount         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LastEmailCCAddress get whether cma-cgm is cc'd or not\n",
    "df['CMA_in_cc'] = df['LastEmailCCAddress'].str.find('cma-cgm') > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers from emails, they are not relevant\n",
    "df[\"LastIncomingEmailContent\"] = df[\"LastIncomingEmailContent\"].str.translate(str.maketrans('', '', digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(\"\\n(.*)\\n\", s)\n",
    "# this returns the email sentence by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have NaN in LastIncomingEmailContent\n",
    "df = df.drop(df[df[\"LastIncomingEmailContent\"].isnull()].index)\n",
    "\n",
    "# reindex, otherwise some indexes are missing\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature: how many emails are exchanged in each conversation, counting Sent:\n",
    "df[\"CountMailsInConversation\"] = [len(re.findall(\"Sent: .* :To:\", str(df[\"LastIncomingEmailContent\"][i]))) for i in range(len(df[\"LastIncomingEmailContent\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just text of the first email. I try doing that by cutting off when there is a salutation\n",
    "# eliminate characters non needed and stopwords\n",
    "\n",
    "# define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "only_text = []\n",
    "for i in range(len(df[\"LastIncomingEmailContent\"])):\n",
    "    \n",
    "    # get text before one of the salutations\n",
    "    text = re.split('Best|Regards|Rgds|Warmest|Warmly|Looking forward|Be well|Yours Truly|Sincerely|Booking ref| \\\n",
    "                      From: .* <.*>' , df.loc[i, \"LastIncomingEmailContent\"], flags=re.IGNORECASE)[0]\n",
    "    \n",
    "    # remove if language is not english (replace by NaN and then remove NaN)\n",
    "    if langid.classify(text) != 'en':\n",
    "        only_text.append(\"\")\n",
    "        continue  # go to next iteration\n",
    "    \n",
    "    # remove these because we don't need them\n",
    "    for ch in ['\\n', '\\xa0']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, '')\n",
    "    \n",
    "    # remove stopwords\n",
    "    text_tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in text_tokens if not word in stopwords.words()])\n",
    "    \n",
    "    # keep everything lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # word lemmatization\n",
    "    lemma_words = [lemmatizer.lemmatize(o) for o in text.split()]\n",
    "    text = \" \".join(lemma_words)\n",
    "                 \n",
    "    only_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to new dataframe column\n",
    "df['only_text'] = only_text\n",
    "\n",
    "# remove nan elements\n",
    "# drop rows that have NaN in only_text\n",
    "df = df.drop(df[df[\"only_text\"].isnull()].index)\n",
    "\n",
    "# reindex, otherwise some indexes are missing\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df.to_csv('/home/azanon/ML_NOAC_NOVA_Extraction_Cleaned_New.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe6f45ecf374f0b90968f2209cadc71235e6ef8554841b35fd27a05b14fd144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
